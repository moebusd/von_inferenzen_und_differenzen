{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Topic Modeling with Binder, Gensim and Mallet\n\nThis notebook implements [Gensim](https://radimrehurek.com/gensim/) and [Mallet](http://mallet.cs.umass.edu/index.php) for topic modeling using the [Binder](https://mybinder.org/) platform. The README is available at the [Binder + Gensim + Mallet Github repository](https://github.com/polsci/binder-gensim-mallet).\n\n## Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check java version - used to make sure OpenJDK installed ok\n!java -version","execution_count":1,"outputs":[{"output_type":"stream","text":"openjdk version \"1.8.0_312\"\r\nOpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~18.04-b07)\r\nOpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# download and unzip mallet\n!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n!unzip -q mallet-2.0.8.zip","execution_count":2,"outputs":[{"output_type":"stream","text":"--2022-06-27 12:27:07--  http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\nResolving mallet.cs.umass.edu (mallet.cs.umass.edu)... 128.119.246.70\nConnecting to mallet.cs.umass.edu (mallet.cs.umass.edu)|128.119.246.70|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://mallet.cs.umass.edu/dist/mallet-2.0.8.zip [following]\n--2022-06-27 12:27:07--  https://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\nConnecting to mallet.cs.umass.edu (mallet.cs.umass.edu)|128.119.246.70|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 16184794 (15M) [application/zip]\nSaving to: ‘mallet-2.0.8.zip’\n\nmallet-2.0.8.zip    100%[===================>]  15.43M  9.93MB/s    in 1.6s    \n\n2022-06-27 12:27:09 (9.93 MB/s) - ‘mallet-2.0.8.zip’ saved [16184794/16184794]\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing we can get some output from mallet - should see a list of Mallet 2.0 commands\n!mallet-2.0.8/bin/mallet","execution_count":3,"outputs":[{"output_type":"stream","text":"Unrecognized command: \r\nMallet 2.0 commands: \r\n\r\n  import-dir         load the contents of a directory into mallet instances (one per file)\r\n  import-file        load a single file into mallet instances (one per line)\r\n  import-svmlight    load SVMLight format data files into Mallet instances\r\n  info               get information about Mallet instances\r\n  train-classifier   train a classifier from Mallet data files\r\n  classify-dir       classify data from a single file with a saved classifier\r\n  classify-file      classify the contents of a directory with a saved classifier\r\n  classify-svmlight  classify data from a single file in SVMLight format\r\n  train-topics       train a topic model from Mallet data files\r\n  infer-topics       use a trained topic model to infer topics for new documents\r\n  evaluate-topics    estimate the probability of new documents under a trained model\r\n  prune              remove features based on frequency or information gain\r\n  split              divide data into testing, training, and validation portions\r\n  bulk-load          for big input files, efficiently prune vocabulary and import docs\r\n\r\nInclude --help with any option for more information\r\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Upload and extract corpus\n\nYou should use Jupyter's file browser to upload a zip file with your corpus. The zip file of the corpus should contain a single directory containing .txt files. Make sure the `path_to_zip_file` is correct below and then run the cell to unzip your corpus. Check Jupyter's file browser to make sure your corpus has been correctly extracted."},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\n\npath_to_zip_file = 'ted-transcripts.zip' # change this to your zip file name\n\nwith zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n    zip_ref.extractall('.')","execution_count":4,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'ted-transcripts.zip'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_44/1441534767.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath_to_zip_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ted-transcripts.zip'\u001b[0m \u001b[0;31m# change this to your zip file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_zip_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ted-transcripts.zip'"]}]},{"metadata":{},"cell_type":"markdown","source":"## Import required libraries for topic modeling"},{"metadata":{"trusted":false},"cell_type":"code","source":"import gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.wrappers import LdaMallet\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom gensim import similarities\n\nimport os.path\nimport re\nimport glob\n\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set the path to the Mallet binary and set the path to the corpus"},{"metadata":{"trusted":false},"cell_type":"code","source":"# you should NOT need to change this \nmallet_path = 'mallet-2.0.8/bin/mallet' \n\n# you need to change this path to the directory containing your corpus of .txt files\ncorpus_path = 'transcripts' ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Functions to load and preprocess the corpus and create the document-term matrix\n\nThe following cell contains functions to load a corpus from a directory of text files, preprocess the corpus and create the bag of words document-term matrix. "},{"metadata":{"trusted":false},"cell_type":"code","source":"def load_data_from_dir(path):\n    file_list = glob.glob(path + '/*.txt')\n\n    # create document list:\n    documents_list = []\n    source_list = []\n    for filename in file_list:\n        with open(filename, 'r', encoding='utf8') as f:\n            text = f.read()\n            f.close()\n            documents_list.append(text)\n            source_list.append(os.path.basename(filename))\n    print(\"Total Number of Documents:\",len(documents_list))\n    return documents_list, source_list\n\ndef preprocess_data(doc_set,extra_stopwords = {}):\n    # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n    # replace all newlines or multiple sequences of spaces with a standard space\n    doc_set = [re.sub('\\s+', ' ', doc) for doc in doc_set]\n    # initialize regex tokenizer\n    tokenizer = RegexpTokenizer(r'\\w+')\n    # create English stop words list\n    en_stop = set(stopwords.words('english'))\n    # add any extra stopwords\n    if (len(extra_stopwords) > 0):\n        en_stop = en_stop.union(extra_stopwords)\n    \n    # list for tokenized documents in loop\n    texts = []\n    # loop through document list\n    for i in doc_set:\n        # clean and tokenize document string\n        raw = i.lower()\n        tokens = tokenizer.tokenize(raw)\n        # remove stop words from tokens\n        stopped_tokens = [i for i in tokens if not i in en_stop]\n        # add tokens to list\n        texts.append(stopped_tokens)\n    return texts\n\ndef prepare_corpus(doc_clean):\n    # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n    dictionary = corpora.Dictionary(doc_clean)\n    \n    dictionary.filter_extremes(no_below=5, no_above=0.5)\n    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n    # generate LDA model\n    return dictionary,doc_term_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and pre-process the corpus\nLoad the corpus, preprocess with additional stop words and output dictionary and document-term matrix."},{"metadata":{"trusted":false},"cell_type":"code","source":"# adjust the path below to wherever you have the transcripts2018 folder\ndocument_list, source_list = load_data_from_dir(corpus_path)\n\n# I've added extra stopwords here in addition to NLTK's stopword list - you could look at adding others.\ndoc_clean = preprocess_data(document_list,{'laughter','applause'})\ndictionary, doc_term_matrix = prepare_corpus(doc_clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LDA model with 30 topics\nThe following cell sets the number of topics we are training the model for. "},{"metadata":{"trusted":false},"cell_type":"code","source":"number_of_topics=30 # adjust this to alter the number of topics\nwords=20 #adjust this to alter the number of words output for the topic below","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following cell runs LDA using Mallet from Gensim using the number_of_topics specified above. This might take a few minutes!"},{"metadata":{"trusted":false},"cell_type":"code","source":"ldamallet30 = LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=number_of_topics, id2word=dictionary, workers=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following cell outputs the topics."},{"metadata":{"trusted":false},"cell_type":"code","source":"ldamallet30.show_topics(num_topics=number_of_topics,num_words=words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert to Gensim model format\nConvert the Mallet model to Gensim format."},{"metadata":{"trusted":false},"cell_type":"code","source":"gensimmodel30 = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get a coherence score"},{"metadata":{"trusted":false},"cell_type":"code","source":"coherencemodel = CoherenceModel(model=gensimmodel30, texts=doc_clean, dictionary=dictionary, coherence='c_v')\nprint (coherencemodel.get_coherence())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get id for specific videos"},{"metadata":{"trusted":false},"cell_type":"code","source":"lookup_doc_id = source_list.index('2017-09-20-zeynep_tufekci_we_re_building_a_dystopia_just_to_make_people_click_on_ads.txt')\nprint('Document ID from lookup:', lookup_doc_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preview a document\n\nPreview a document - you can change the doc_id to view another document."},{"metadata":{"trusted":false},"cell_type":"code","source":"doc_id = lookup_doc_id # index of document to explore - this can be an id number or set to lookup_doc_id\nprint(re.sub('\\s+', ' ', document_list[doc_id])) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Output the distribution of topics for the document\n\nThe next cell outputs the distribution of topics on the document specified above."},{"metadata":{"trusted":false},"cell_type":"code","source":"document_topics = gensimmodel30.get_document_topics(doc_term_matrix[doc_id])\ndocument_topics = sorted(document_topics, key=lambda x: x[1], reverse=True) # sorts document topics\n\nfor topic, prop in document_topics:\n    topic_words = [word[0] for word in gensimmodel30.show_topic(topic, 10)]\n    print (\"%.2f\" % prop, topic, topic_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Find similar documents\nThis will find the 5 most similar documents to the document specified above based on their topic distribution."},{"metadata":{"trusted":false},"cell_type":"code","source":"# gensimmodel30[doc_term_matrix] below represents the documents in the corpus in LDA vector space\nlda_index = similarities.MatrixSimilarity(gensimmodel30[doc_term_matrix])\n\n# query for our doc_id from above\nsimilarity_index = lda_index[gensimmodel30[doc_term_matrix[doc_id]]]\n\n# Sort the similarity index\nsimilarity_index = sorted(enumerate(similarity_index), key=lambda item: -item[1])\n\nfor i in range(1,6): \n    document_id, similarity_score = similarity_index[i]\n\n    print('Document Index:',document_id)\n    print('Document:', source_list[document_id])\n    print('Similarity Score:',similarity_score)\n    \n    print(re.sub('\\s+', ' ', document_list[document_id][:500]), '...') # preview first 500 characters\n    \n    document_topics = gensimmodel30[doc_term_matrix[document_id]]\n    document_topics = sorted(document_topics, key=lambda x: x[1], reverse=True)\n    for topic, prop in document_topics:\n        topic_words = [word[0] for word in gensimmodel30.show_topic(topic, 10)]\n        print (\"%.2f\" % prop, topic, topic_words)\n    \n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}